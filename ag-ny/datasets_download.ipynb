{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOsy4xTx692fXENfWkiEBsK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"UQzyIAya3mbQ"},"outputs":[],"source":["# Imports and Setup\n","import os, re, json, math, time, textwrap, pathlib, itertools\n","from pathlib import Path\n","from typing import Optional, Dict, Any, List\n","import requests\n","import pandas as pd\n","\n","!mkdir -p data\n","\n","DATA_ROOT = Path(\"/content/agny-SP/data\").resolve()"]},{"cell_type":"code","source":["# Helpers\n","# --Log\n","def log(msg: str):\n","    print(f\"[DL] {msg}\")\n","\n","# --Save\n","def save_bytes(content: bytes, path: Path):\n","    path.parent.mkdir(parents=True, exist_ok=True)\n","    with open(path, \"wb\") as f:\n","        f.write(content)\n","    log(f\"Saved: {path} ({len(content):,} bytes)\")\n","\n","# --HTTP GET\n","def http_get(url: str, headers: Optional[Dict[str,str]] = None, params: Optional[Dict[str,Any]] = None, retries: int = 4, sleep: float = 1.0):\n","    last_err = None\n","    for i in range(retries):\n","        try:\n","            r = requests.get(url, headers=headers, params=params, timeout=60)\n","            r.raise_for_status()\n","            return r\n","        except Exception as e:\n","            last_err = e\n","            log(f\"GET failed ({i+1}/{retries}) for {url}: {e}\")\n","            time.sleep(sleep * (i+1))\n","    raise last_err\n","\n","# --Pandas\n","def excel_to_csvs(xlsx_path: Path, out_dir: Path, preview_rows: int = 3):\n","    out_dir.mkdir(parents=True, exist_ok=True)\n","    xl = pd.ExcelFile(xlsx_path)\n","    shapes = {}\n","    for sheet in xl.sheet_names:\n","        df = xl.parse(sheet_name=sheet)\n","        safe_sheet = re.sub(r'[^A-Za-z0-9._-]+', '_', sheet).strip('_')\n","        csv_path = out_dir / f\"{xlsx_path.stem}__{safe_sheet}.csv\"\n","        df.to_csv(csv_path, index=False)\n","        shapes[sheet] = (df.shape[0], df.shape[1])\n","        log(f\"  - wrote {csv_path.name}: {df.shape[0]} rows x {df.shape[1]} cols\")\n","        if preview_rows > 0:\n","            with pd.option_context(\"display.width\", 160, \"display.max_columns\", None):\n","                log(f\"    Preview of '{sheet}':\\n{df.head(preview_rows)}\")\n","    return shapes\n"],"metadata":{"id":"pnZ2PDxCB0Fq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Dataset Source Access\n","\n","\n","*   USGS ScienceBase\n","*   NYC Open Data (Socrata)\n","*   USDA\n","\n"],"metadata":{"id":"B9KDVjbeCOor"}},{"cell_type":"markdown","source":["###USGS ScienceBase"],"metadata":{"id":"Tci4cxmtC54a"}},{"cell_type":"code","source":["# ScienceBase\n","# --fetch item JSON, pull matching file\n","def sciencebase_fetch(item_id: str) -> Dict[str, Any]:\n","    url = f\"https://www.sciencebase.gov/catalog/item/{item_id}?format=json\"\n","    r = http_get(url)\n","    return r.json()\n","\n","def sciencebase_download_excel(item_id: str, out_dir: Path, filename_match: Optional[str] = None):\n","    meta = sciencebase_fetch(item_id)\n","    files = meta.get(\"files\") or []\n","    if not files:\n","        raise RuntimeError(f\"No files listed for ScienceBase item {item_id}\")\n","\n","    downloaded = []\n","    for f in files:\n","        name = f.get(\"name\") or \"\"\n","        url  = f.get(\"url\")  or \"\"\n","        if not name or not url:\n","            continue\n","        if filename_match and filename_match.lower() not in name.lower():\n","            continue\n","        if not name.lower().endswith((\".xlsx\", \".xls\")):\n","            continue\n","\n","        out_dir.mkdir(parents=True, exist_ok=True)\n","        out_path = out_dir / name\n","        resp = http_get(url)\n","        save_bytes(resp.content, out_path)\n","        downloaded.append(out_path)\n","    if not downloaded:\n","        raise RuntimeError(f\"No matching Excel files found for item {item_id} (filter={filename_match})\")\n","    return downloaded\n"],"metadata":{"id":"Sl0ZXtbdC9dc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Socrata (NYC Open Data)"],"metadata":{"id":"iioXWAZ7DSP3"}},{"cell_type":"code","source":["# NYC Open Data (Socrata)\n","# (for fewer rate limits) SOCRATA_APP_TOKEN = os.getenv(\"NYC_OPENDATA_APP_TOKEN\", None)\n","\n","def socrata_to_csv(resource_id: str, out_csv: Path, domain: str = \"data.cityofnewyork.us\", chunk: int = 50_000):\n","    \"\"\"\n","    Paginate the SODA API and write a single CSV with all rows.\n","    \"\"\"\n","    headers = {}\n","    if SOCRATA_APP_TOKEN:\n","        headers[\"X-App-Token\"] = SOCRATA_APP_TOKEN\n","\n","    out_csv.parent.mkdir(parents=True, exist_ok=True)\n","    first = True\n","    offset = 0\n","    total_rows = 0\n","\n","    while True:\n","        params = {\n","            \"$limit\": chunk,\n","            \"$offset\": offset\n","        }\n","        url = f\"https://{domain}/resource/{resource_id}.json\"\n","        resp = http_get(url, headers=headers, params=params)\n","        rows = resp.json()\n","        if not rows:\n","            break\n","        df = pd.DataFrame(rows)\n","        mode = \"w\" if first else \"a\"\n","        header = first\n","        df.to_csv(out_csv, index=False, mode=mode, header=header)\n","        total_rows += len(df)\n","        log(f\"{resource_id}: fetched {len(df):,} rows (offset {offset:,})\")\n","        first = False\n","        offset += chunk\n","\n","\n","    if total_rows > 0:\n","        df_head = pd.read_csv(out_csv, nrows=5)\n","        log(f\"Wrote {total_rows:,} rows to {out_csv.name}. Preview:\\n{df_head}\")\n","    else:\n","        log(f\"No rows found for {resource_id}\")\n"],"metadata":{"id":"Cv6DsVZrDXQl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MANIFEST = {\n","    # --- USGS ScienceBase items (Hydrologic Data Summaries) ---\n","    \"USGS_LI_PineBarrens_2020\": {\n","        \"type\": \"sciencebase\",\n","        \"item_id\": \"62d9650bd34e2842e1edcf5a\",\n","        \"file_contains\": \"LIPineBarrens_2020_DataRelease.xlsx\"  # hint for filtering\n","    },\n","    \"USGS_LI_PineBarrens_2021\": {\n","        \"type\": \"sciencebase\",\n","        \"item_id\": \"63208bc8d34e71c6d67aa87e\",\n","        \"file_contains\": \"LIPineBarrens_2021_DataRelease.xlsx\"\n","    },\n","    \"USGS_LI_PineBarrens_2022\": {\n","        \"type\": \"sciencebase\",\n","        \"item_id\": \"64c7f088d34e70357a349e3b\",\n","        \"file_contains\": \"LIPineBarrens_2022_DataRelease.xlsx\"\n","    },\n","\n","    # --- Bronx River PFAS/pesticides/pharmaceuticals (ScienceBase) ---\n","    \"USGS_BronxRiver_2019_PFAS_Pest_Pharma\": {\n","        \"type\": \"sciencebase\",\n","        \"item_id\": \"62d96512d34e2842e1edcf5e\",\n","        \"file_contains\": \"PFAS_pesticides_pharmaceuticals_BronxRiver2019_NYC_NY_all_data.xlsx\"\n","    },\n","\n","    # --- Western NY & Mohawk River Basin Groundwater 2021 (ScienceBase) ---\n","    \"USGS_WNY_Mohawk_2021\": {\n","        \"type\": \"sciencebase\",\n","        \"item_id\": \"65203b86d34e44db0e2e4406\",\n","        \"file_contains\": \".xlsx\"  # catch the main Excel\n","    },\n","\n","    # --- Long Island shallow groundwater 2016-2018 (ScienceBase) ---\n","    \"USGS_LI_ShallowGW_2016_2018\": {\n","        \"type\": \"sciencebase\",\n","        \"item_id\": \"5e83a6b4e4b01d50927b62a9\",\n","        \"file_contains\": \".xlsx\"\n","    },\n","\n","    # --- NYC Open Data (Socrata) ---\n","    \"NYC_DEP_Distribution_WQ\": {\n","        \"type\": \"socrata\",\n","        \"resource_id\": \"bkwf-xfky\",\n","        \"outfile\": \"Drinking_Water_Quality_Distribution_Monitoring_Data.csv\"\n","    },\n","    \"NYC_DEP_Watershed_WQ\": {\n","        \"type\": \"socrata\",\n","        \"resource_id\": \"y43c-5n92\",\n","        \"outfile\": \"Watershed_Water_Quality_Data.csv\"\n","    },\n","    \"NYC_DEP_Crypto_Giardia\": {\n","        \"type\": \"socrata\",\n","        \"resource_id\": \"x2s6-6d2j\",\n","        \"outfile\": \"DEP_Cryptosporidium_And_Giardia_Data_Set.csv\"\n","    },\n","\n","    # --- USDA/NASS CSV (direct link downloads you provided) ---\n","    \"USDA_NASS_Regional_Crop_Yield\": {\n","        \"type\": \"direct_csv\",\n","        # try the 'printable' link first; if it fails, try the results GUID link\n","        \"urls\": [\n","            \"https://quickstats.nass.usda.gov/data/printable/F749339A-E2BC-3A10-BFCC-91B6686691E0\",\n","            \"https://quickstats.nass.usda.gov/results/3D668DBB-3D36-3013-8052-B1F3C7580171\"\n","        ],\n","        \"outfile\": \"USDA_Regional_Crop_Yield_Data.csv\"\n","    },\n","}\n"],"metadata":{"id":"7hO00GNiDyJK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_manifest(manifest: Dict[str, Dict[str, Any]], root: Path = DATA_ROOT):\n","    summary = {}\n","\n","    for key, cfg in manifest.items():\n","        log(f\"=== {key} ===\")\n","        out_dir = root / key\n","        kind = cfg[\"type\"]\n","\n","        if kind == \"sciencebase\":\n","            item_id = cfg[\"item_id\"]\n","            contains = cfg.get(\"file_contains\")\n","            xls_paths = sciencebase_download_excel(item_id, out_dir, filename_match=contains)\n","            sheet_shapes_all = {}\n","            for p in xls_paths:\n","                csv_dir = out_dir / \"csv\"\n","                shapes = excel_to_csvs(p, csv_dir)\n","                sheet_shapes_all[p.name] = shapes\n","            summary[key] = {\n","                \"excel_files\": [str(p) for p in xls_paths],\n","                \"sheet_shapes\": sheet_shapes_all\n","            }\n","\n","        elif kind == \"socrata\":\n","            rid = cfg[\"resource_id\"]\n","            outfile = cfg[\"outfile\"]\n","            out_csv = out_dir / outfile\n","            socrata_to_csv(rid, out_csv)\n","            summary[key] = {\"csv\": str(out_csv)}\n","\n","        elif kind == \"direct_csv\":\n","            urls = cfg[\"urls\"]\n","            outfile = cfg[\"outfile\"]\n","            out_csv = out_dir / outfile\n","            out_dir.mkdir(parents=True, exist_ok=True)\n","            last_err = None\n","            for u in urls:\n","                try:\n","                    r = http_get(u)\n","                    # Some QuickStats \"printable\" links return text/HTML – try to coerce to CSV if it is CSV.\n","                    # If it's HTML, pandas can often still read the table; but we prefer raw content save first.\n","                    content = r.content\n","                    # Quick sanity: if comma-heavy and linebreaks exist, we’ll just save as CSV.\n","                    save_bytes(content, out_csv)\n","                    # optional: try reading few rows to confirm\n","                    try:\n","                        df = pd.read_csv(out_csv, nrows=5)\n","                        log(f\"Preview of {outfile}:\\n{df}\")\n","                    except Exception as e:\n","                        log(f\"Note: Could not parse preview as CSV yet ({e}). File still saved.\")\n","                    last_err = None\n","                    break\n","                except Exception as e:\n","                    last_err = e\n","                    log(f\"Direct CSV download failed for {u}: {e}\")\n","            if last_err:\n","                raise last_err\n","            summary[key] = {\"csv\": str(out_csv)}\n","        else:\n","            log(f\"Unknown type: {kind}\")\n","\n","    return summary\n","\n","summary = run_manifest(MANIFEST)\n","log(\"\\n=== DONE ===\")\n","print(json.dumps(summary, indent=2)[:2000])  # trim preview\n"],"metadata":{"id":"l7GLtWmnEV33"},"execution_count":null,"outputs":[]}]}